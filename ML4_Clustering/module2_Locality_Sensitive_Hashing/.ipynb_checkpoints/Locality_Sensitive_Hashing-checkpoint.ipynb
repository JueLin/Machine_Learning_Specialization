{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality Sensitive Hashing (LSH) provides for a fast, efficient approximate nearest neighbor search. The algorithm scales well with respect to the number of data points as well as dimensions.\n",
    "\n",
    "In this assignment, you will\n",
    "* Implement the LSH algorithm for approximate nearest neighbor search\n",
    "* Examine the accuracy for different documents by comparing against brute force search, and also contrast runtimes\n",
    "* Explore the role of the algorithmâ€™s tuning parameters in the accuracy of the method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will check if you have the correct version of GraphLab Create. Any version later than 1.8.5 will do. To upgrade, read [this page](https://turi.com/download/upgrade-graphlab-create.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to jue.lin@u.northwestern.edu and will expire on November 09, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1485919296.log\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import graphlab\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "'''Check GraphLab Create version'''\n",
    "from distutils.version import StrictVersion\n",
    "assert (StrictVersion(graphlab.version) >= StrictVersion('1.8.5')), 'GraphLab Create must be version 1.8.5 or later.'\n",
    "\n",
    "'''compute norm of a sparse vector\n",
    "   Thanks to: Jaiyam Sharma'''\n",
    "def norm(x):\n",
    "    sum_sq=x.dot(x.T)\n",
    "    norm=np.sqrt(sum_sq)\n",
    "    return(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load in the Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki = graphlab.SFrame('people_wiki.gl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">URI</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Digby_Morrell&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Digby Morrell</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">digby morrell born 10<br>october 1979 is a former ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Alfred_J._Lewy&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Alfred J. Lewy</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">alfred j lewy aka sandy<br>lewy graduated from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Harpdog_Brown&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Harpdog Brown</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">harpdog brown is a singer<br>and harmonica player who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Franz_Rottensteiner&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Franz Rottensteiner</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">franz rottensteiner born<br>in waidmannsfeld lower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/G-Enka&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">G-Enka</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">henry krvits born 30<br>december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Sam_Henderson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sam Henderson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sam henderson born<br>october 18 1969 is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Aaron_LaCrate&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Aaron LaCrate</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">aaron lacrate is an<br>american music producer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Trevor_Ferguson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Trevor Ferguson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">trevor ferguson aka john<br>farrow born 11 november ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Grant_Nelson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Grant Nelson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">grant nelson born 27<br>april 1971 in london  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Cathy_Caruth&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Cathy Caruth</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">cathy caruth born 1955 is<br>frank h t rhodes ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tURI\tstr\n",
       "\tname\tstr\n",
       "\ttext\tstr\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------------------------------+---------------------+\n",
       "|              URI              |         name        |\n",
       "+-------------------------------+---------------------+\n",
       "| <http://dbpedia.org/resour... |    Digby Morrell    |\n",
       "| <http://dbpedia.org/resour... |    Alfred J. Lewy   |\n",
       "| <http://dbpedia.org/resour... |    Harpdog Brown    |\n",
       "| <http://dbpedia.org/resour... | Franz Rottensteiner |\n",
       "| <http://dbpedia.org/resour... |        G-Enka       |\n",
       "| <http://dbpedia.org/resour... |    Sam Henderson    |\n",
       "| <http://dbpedia.org/resour... |    Aaron LaCrate    |\n",
       "| <http://dbpedia.org/resour... |   Trevor Ferguson   |\n",
       "| <http://dbpedia.org/resour... |     Grant Nelson    |\n",
       "| <http://dbpedia.org/resour... |     Cathy Caruth    |\n",
       "+-------------------------------+---------------------+\n",
       "+-------------------------------+\n",
       "|              text             |\n",
       "+-------------------------------+\n",
       "| digby morrell born 10 octo... |\n",
       "| alfred j lewy aka sandy le... |\n",
       "| harpdog brown is a singer ... |\n",
       "| franz rottensteiner born i... |\n",
       "| henry krvits born 30 decem... |\n",
       "| sam henderson born october... |\n",
       "| aaron lacrate is an americ... |\n",
       "| trevor ferguson aka john f... |\n",
       "| grant nelson born 27 april... |\n",
       "| cathy caruth born 1955 is ... |\n",
       "+-------------------------------+\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, let us assign a unique ID to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">URI</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Digby_Morrell&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Digby Morrell</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">digby morrell born 10<br>october 1979 is a former ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Alfred_J._Lewy&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Alfred J. Lewy</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">alfred j lewy aka sandy<br>lewy graduated from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Harpdog_Brown&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Harpdog Brown</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">harpdog brown is a singer<br>and harmonica player who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Franz_Rottensteiner&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Franz Rottensteiner</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">franz rottensteiner born<br>in waidmannsfeld lower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/G-Enka&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">G-Enka</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">henry krvits born 30<br>december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Sam_Henderson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sam Henderson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sam henderson born<br>october 18 1969 is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Aaron_LaCrate&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Aaron LaCrate</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">aaron lacrate is an<br>american music producer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Trevor_Ferguson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Trevor Ferguson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">trevor ferguson aka john<br>farrow born 11 november ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Grant_Nelson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Grant Nelson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">grant nelson born 27<br>april 1971 in london  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Cathy_Caruth&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Cathy Caruth</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">cathy caruth born 1955 is<br>frank h t rhodes ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[59071 rows x 4 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tid\tint\n",
       "\tURI\tstr\n",
       "\tname\tstr\n",
       "\ttext\tstr\n",
       "\n",
       "Rows: 59071\n",
       "\n",
       "Data:\n",
       "+----+-------------------------------+---------------------+\n",
       "| id |              URI              |         name        |\n",
       "+----+-------------------------------+---------------------+\n",
       "| 0  | <http://dbpedia.org/resour... |    Digby Morrell    |\n",
       "| 1  | <http://dbpedia.org/resour... |    Alfred J. Lewy   |\n",
       "| 2  | <http://dbpedia.org/resour... |    Harpdog Brown    |\n",
       "| 3  | <http://dbpedia.org/resour... | Franz Rottensteiner |\n",
       "| 4  | <http://dbpedia.org/resour... |        G-Enka       |\n",
       "| 5  | <http://dbpedia.org/resour... |    Sam Henderson    |\n",
       "| 6  | <http://dbpedia.org/resour... |    Aaron LaCrate    |\n",
       "| 7  | <http://dbpedia.org/resour... |   Trevor Ferguson   |\n",
       "| 8  | <http://dbpedia.org/resour... |     Grant Nelson    |\n",
       "| 9  | <http://dbpedia.org/resour... |     Cathy Caruth    |\n",
       "+----+-------------------------------+---------------------+\n",
       "+-------------------------------+\n",
       "|              text             |\n",
       "+-------------------------------+\n",
       "| digby morrell born 10 octo... |\n",
       "| alfred j lewy aka sandy le... |\n",
       "| harpdog brown is a singer ... |\n",
       "| franz rottensteiner born i... |\n",
       "| henry krvits born 30 decem... |\n",
       "| sam henderson born october... |\n",
       "| aaron lacrate is an americ... |\n",
       "| trevor ferguson aka john f... |\n",
       "| grant nelson born 27 april... |\n",
       "| cathy caruth born 1955 is ... |\n",
       "+-------------------------------+\n",
       "[59071 rows x 4 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = wiki.add_row_number()\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract TF-IDF matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use GraphLab Create to compute a TF-IDF representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">URI</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tf_idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Digby_Morrell&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Digby Morrell</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">digby morrell born 10<br>october 1979 is a former ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'selection':<br>3.836578553093086, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Alfred_J._Lewy&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Alfred J. Lewy</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">alfred j lewy aka sandy<br>lewy graduated from ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'precise':<br>6.44320060695519, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Harpdog_Brown&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Harpdog Brown</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">harpdog brown is a singer<br>and harmonica player who ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'just':<br>2.7007299687108643, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Franz_Rottensteiner&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Franz Rottensteiner</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">franz rottensteiner born<br>in waidmannsfeld lower ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.6431112434912472, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/G-Enka&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">G-Enka</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">henry krvits born 30<br>december 1974 in tallinn ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'they':<br>1.8993401178193898, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Sam_Henderson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sam Henderson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sam henderson born<br>october 18 1969 is an ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'currently':<br>1.637088969126014, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Aaron_LaCrate&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Aaron LaCrate</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">aaron lacrate is an<br>american music producer ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'exclusive':<br>10.455187230695827, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Trevor_Ferguson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Trevor Ferguson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">trevor ferguson aka john<br>farrow born 11 november ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'taxi':<br>6.0520214560945025, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Grant_Nelson&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Grant Nelson</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">grant nelson born 27<br>april 1971 in london  ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'houston':<br>3.935505942157149, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt;http://dbpedia.org/resou<br>rce/Cathy_Caruth&gt; ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Cathy Caruth</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">cathy caruth born 1955 is<br>frank h t rhodes ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'phenomenon':<br>5.750053426395245, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[59071 rows x 5 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tid\tint\n",
       "\tURI\tstr\n",
       "\tname\tstr\n",
       "\ttext\tstr\n",
       "\ttf_idf\tdict\n",
       "\n",
       "Rows: 59071\n",
       "\n",
       "Data:\n",
       "+----+-------------------------------+---------------------+\n",
       "| id |              URI              |         name        |\n",
       "+----+-------------------------------+---------------------+\n",
       "| 0  | <http://dbpedia.org/resour... |    Digby Morrell    |\n",
       "| 1  | <http://dbpedia.org/resour... |    Alfred J. Lewy   |\n",
       "| 2  | <http://dbpedia.org/resour... |    Harpdog Brown    |\n",
       "| 3  | <http://dbpedia.org/resour... | Franz Rottensteiner |\n",
       "| 4  | <http://dbpedia.org/resour... |        G-Enka       |\n",
       "| 5  | <http://dbpedia.org/resour... |    Sam Henderson    |\n",
       "| 6  | <http://dbpedia.org/resour... |    Aaron LaCrate    |\n",
       "| 7  | <http://dbpedia.org/resour... |   Trevor Ferguson   |\n",
       "| 8  | <http://dbpedia.org/resour... |     Grant Nelson    |\n",
       "| 9  | <http://dbpedia.org/resour... |     Cathy Caruth    |\n",
       "+----+-------------------------------+---------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|              text             |             tf_idf            |\n",
       "+-------------------------------+-------------------------------+\n",
       "| digby morrell born 10 octo... | {'selection': 3.8365785530... |\n",
       "| alfred j lewy aka sandy le... | {'precise': 6.443200606955... |\n",
       "| harpdog brown is a singer ... | {'just': 2.700729968710864... |\n",
       "| franz rottensteiner born i... | {'all': 1.6431112434912472... |\n",
       "| henry krvits born 30 decem... | {'they': 1.899340117819389... |\n",
       "| sam henderson born october... | {'currently': 1.6370889691... |\n",
       "| aaron lacrate is an americ... | {'exclusive': 10.455187230... |\n",
       "| trevor ferguson aka john f... | {'taxi': 6.052021456094502... |\n",
       "| grant nelson born 27 april... | {'houston': 3.935505942157... |\n",
       "| cathy caruth born 1955 is ... | {'phenomenon': 5.750053426... |\n",
       "+-------------------------------+-------------------------------+\n",
       "[59071 rows x 5 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki['tf_idf'] = graphlab.text_analytics.tf_idf(wiki['text'])\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the remainder of the assignment, we will use sparse matrices. Sparse matrices are [matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics%29 ) that have a small number of nonzero entries. A good data structure for sparse matrices would only store the nonzero entries to save space and speed up computation. SciPy provides a highly-optimized library for sparse matrices. Many matrix operations available for NumPy arrays are also available for SciPy sparse matrices.\n",
    "\n",
    "We first convert the TF-IDF column (in dictionary format) into the SciPy sparse matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method stack in module graphlab.data_structures.sframe:\n",
      "\n",
      "stack(self, column_name, new_column_name=None, drop_na=False, new_column_type=None) method of graphlab.data_structures.sframe.SFrame instance\n",
      "    Convert a \"wide\" column of an SFrame to one or two \"tall\" columns by\n",
      "    stacking all values.\n",
      "    \n",
      "    The stack works only for columns of dict, list, or array type.  If the\n",
      "    column is dict type, two new columns are created as a result of\n",
      "    stacking: one column holds the key and another column holds the value.\n",
      "    The rest of the columns are repeated for each key/value pair.\n",
      "    \n",
      "    If the column is array or list type, one new column is created as a\n",
      "    result of stacking. With each row holds one element of the array or list\n",
      "    value, and the rest columns from the same original row repeated.\n",
      "    \n",
      "    The new SFrame includes the newly created column and all columns other\n",
      "    than the one that is stacked.\n",
      "    \n",
      "    Parameters\n",
      "    --------------\n",
      "    column_name : str\n",
      "        The column to stack. This column must be of dict/list/array type\n",
      "    \n",
      "    new_column_name : str | list of str, optional\n",
      "        The new column name(s). If original column is list/array type,\n",
      "        new_column_name must a string. If original column is dict type,\n",
      "        new_column_name must be a list of two strings. If not given, column\n",
      "        names are generated automatically.\n",
      "    \n",
      "    drop_na : boolean, optional\n",
      "        If True, missing values and empty list/array/dict are all dropped\n",
      "        from the resulting column(s). If False, missing values are\n",
      "        maintained in stacked column(s).\n",
      "    \n",
      "    new_column_type : type | list of types, optional\n",
      "        The new column types. If original column is a list/array type\n",
      "        new_column_type must be a single type, or a list of one type. If\n",
      "        original column is of dict type, new_column_type must be a list of\n",
      "        two types. If not provided, the types are automatically inferred\n",
      "        from the first 100 values of the SFrame.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : SFrame\n",
      "        A new SFrame that contains newly stacked column(s) plus columns in\n",
      "        original SFrame other than the stacked column.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    unstack\n",
      "    \n",
      "    Examples\n",
      "    ---------\n",
      "    Suppose 'sf' is an SFrame that contains a column of dict type:\n",
      "    \n",
      "    >>> sf = graphlab.SFrame({'topic':[1,2,3,4],\n",
      "    ...                       'words': [{'a':3, 'cat':2},\n",
      "    ...                                 {'a':1, 'the':2},\n",
      "    ...                                 {'the':1, 'dog':3},\n",
      "    ...                                 {}]\n",
      "    ...                      })\n",
      "    +-------+----------------------+\n",
      "    | topic |        words         |\n",
      "    +-------+----------------------+\n",
      "    |   1   |  {'a': 3, 'cat': 2}  |\n",
      "    |   2   |  {'a': 1, 'the': 2}  |\n",
      "    |   3   | {'the': 1, 'dog': 3} |\n",
      "    |   4   |          {}          |\n",
      "    +-------+----------------------+\n",
      "    [4 rows x 2 columns]\n",
      "    \n",
      "    Stack would stack all keys in one column and all values in another\n",
      "    column:\n",
      "    \n",
      "    >>> sf.stack('words', new_column_name=['word', 'count'])\n",
      "    +-------+------+-------+\n",
      "    | topic | word | count |\n",
      "    +-------+------+-------+\n",
      "    |   1   |  a   |   3   |\n",
      "    |   1   | cat  |   2   |\n",
      "    |   2   |  a   |   1   |\n",
      "    |   2   | the  |   2   |\n",
      "    |   3   | the  |   1   |\n",
      "    |   3   | dog  |   3   |\n",
      "    |   4   | None |  None |\n",
      "    +-------+------+-------+\n",
      "    [7 rows x 3 columns]\n",
      "    \n",
      "    Observe that since topic 4 had no words, an empty row is inserted.\n",
      "    To drop that row, set dropna=True in the parameters to stack.\n",
      "    \n",
      "    Suppose 'sf' is an SFrame that contains a user and his/her friends,\n",
      "    where 'friends' columns is an array type. Stack on 'friends' column\n",
      "    would create a user/friend list for each user/friend pair:\n",
      "    \n",
      "    >>> sf = graphlab.SFrame({'topic':[1,2,3],\n",
      "    ...                       'friends':[[2,3,4], [5,6],\n",
      "    ...                                  [4,5,10,None]]\n",
      "    ...                      })\n",
      "    >>> sf\n",
      "    +-------+------------------+\n",
      "    | topic |     friends      |\n",
      "    +-------+------------------+\n",
      "    |  1    |     [2, 3, 4]    |\n",
      "    |  2    |      [5, 6]      |\n",
      "    |  3    | [4, 5, 10, None] |\n",
      "    +----- -+------------------+\n",
      "    [3 rows x 2 columns]\n",
      "    \n",
      "    >>> sf.stack('friends', new_column_name='friend')\n",
      "    +------+--------+\n",
      "    | user | friend |\n",
      "    +------+--------+\n",
      "    |  1   |  2     |\n",
      "    |  1   |  3     |\n",
      "    |  1   |  4     |\n",
      "    |  2   |  5     |\n",
      "    |  2   |  6     |\n",
      "    |  3   |  4     |\n",
      "    |  3   |  5     |\n",
      "    |  3   |  10    |\n",
      "    |  3   |  None  |\n",
      "    +------+--------+\n",
      "    [9 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wiki.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module graphlab.toolkits.feature_engineering._one_hot_encoder:\n",
      "\n",
      "class OneHotEncoder(graphlab.toolkits.feature_engineering._feature_engineering.Transformer)\n",
      " |  Encode a collection of categorical features using a *1-of-K* encoding scheme.\n",
      " |  \n",
      " |  Input columns to the one-hot-encoder must by of type *int*, *string*,\n",
      " |  *dict*, or *list*. The transformed output is a column of type dictionary\n",
      " |  (`max_categories` per column dimension sparse vector) where the key\n",
      " |  corresponds to the index of the categorical variable and the value is `1`.\n",
      " |  \n",
      " |  The behaviour of the one-hot-encoder for each input data column type is as\n",
      " |  follows. (see :func:`~graphlab.feature_engineering.OneHotEncoder.transform`\n",
      " |  for examples of the same).\n",
      " |  \n",
      " |  \n",
      " |  * **string** : The key in the output dictionary is the string category and\n",
      " |    the value is 1.\n",
      " |  \n",
      " |  * **int** : Behave similar to *string* columns.\n",
      " |  \n",
      " |  * **list** : Each value in the list is treated like an individual string.\n",
      " |    Hence, a *list* of categorical variables can be used to represent a\n",
      " |    feature where all categories in the list are simultaneously `hot`.\n",
      " |  \n",
      " |  * **dict** : They key of the dictionary is treated as a `namespace` and the\n",
      " |    value is treated as a `sub-category` in the `namespace`. The categorical\n",
      " |    variable being encoded in this case is a combination of the `namespace`\n",
      " |    and the `sub-category`.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  features : list[str] | str | None, optional\n",
      " |      Name(s) of feature column(s) to be transformed. If set to None, then all\n",
      " |      columns are used.\n",
      " |  \n",
      " |  excluded_features : list[str] | str | None, optional\n",
      " |      Name(s) of feature columns in the input dataset to be ignored. Either\n",
      " |      `excluded_features` or `features` can be passed, but not both.\n",
      " |  \n",
      " |  max_categories: int, optional\n",
      " |      The maximum number of categories (per feature column) to use in the\n",
      " |      encoding. If the number of unique categories in a column exceed\n",
      " |      `max_categories`, then only the most frequent used categories are retained.\n",
      " |      If set to None, then all categories in the column are used.\n",
      " |  \n",
      " |  output_column_name : str, optional\n",
      " |      The name of the output column. If the column already exists, then a\n",
      " |      suffix is appended to the name.\n",
      " |  \n",
      " |  Returns\n",
      " |  -------\n",
      " |  out : OneHotEncoder\n",
      " |      A OneHotEncoder object which is initialized with the defined\n",
      " |      parameters.\n",
      " |  \n",
      " |  Notes\n",
      " |  -------\n",
      " |  - `None` values are treated as separate categories and are encoded along with the rest of the values.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  graphlab.toolkits.feature_engineering._count_thresholder.OneHotEnconder, graphlab.toolkits.feature_engineering.create\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  .. sourcecode:: python\n",
      " |  \n",
      " |      # Create data.\n",
      " |      >>> sf = graphlab.SFrame({'a': [1,2,3], 'b' : [2,3,4]})\n",
      " |  \n",
      " |      # Create a one-hot encoder on the features ['a', 'b'].\n",
      " |      >>> from graphlab.toolkits.feature_engineering import OneHotEncoder\n",
      " |      >>> encoder = graphlab.feature_engineering.create(sf,\n",
      " |                          OneHotEncoder(features = ['a', 'b']))\n",
      " |  \n",
      " |      # Transform data.\n",
      " |      >>> transformed_sf = encoder.transform(sf)\n",
      " |      Columns:\n",
      " |      encoded_features        dict\n",
      " |  \n",
      " |      Rows: 3\n",
      " |  \n",
      " |      Data:\n",
      " |      +------------------+\n",
      " |      | encoded_features |\n",
      " |      +------------------+\n",
      " |      |   {0: 1, 3: 1}   |\n",
      " |      |   {1: 1, 4: 1}   |\n",
      " |      |   {2: 1, 5: 1}   |\n",
      " |      +------------------+\n",
      " |      [3 rows x 1 columns]\n",
      " |  \n",
      " |      # Save the transformer.\n",
      " |      >>> encoder.save('save-path')\n",
      " |  \n",
      " |      # Return the indices in the encoding.\n",
      " |      >>> encoder['feature_encoding']\n",
      " |      Columns:\n",
      " |              feature str\n",
      " |              category        str\n",
      " |              index   int\n",
      " |  \n",
      " |      Rows: 6\n",
      " |  \n",
      " |      Data:\n",
      " |      +---------+----------+-------+\n",
      " |      | feature | category | index |\n",
      " |      +---------+----------+-------+\n",
      " |      |    a    |    1     |   0   |\n",
      " |      |    a    |    2     |   1   |\n",
      " |      |    a    |    3     |   2   |\n",
      " |      |    b    |    2     |   3   |\n",
      " |      |    b    |    3     |   4   |\n",
      " |      |    b    |    4     |   5   |\n",
      " |      +---------+----------+-------+\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      graphlab.toolkits.feature_engineering._feature_engineering.Transformer\n",
      " |      graphlab.toolkits.feature_engineering._feature_engineering.TransformerBase\n",
      " |      graphlab.toolkits._model.CustomModel\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, features=None, excluded_features=None, max_categories=None, output_column_name='encoded_features')\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string description of the model, including a description of\n",
      " |      the training data, training statistics, and model hyper-parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : string\n",
      " |          A description of the model.\n",
      " |  \n",
      " |  fit(self, data)\n",
      " |      Fit a transformer using the SFrame `data`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : SFrame\n",
      " |          The data used to fit the transformer.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self (A fitted version of the object)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      transform, fit_transform\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: python\n",
      " |      \n",
      " |      \n",
      " |          # Create the data\n",
      " |          >>> sf = graphlab.SFrame({'a' : [1,2,3,2,3], 'b' : [2,3,4,2,3]})\n",
      " |      \n",
      " |          # Use the top-2 most frequent categories per column for features ['a', 'b'].\n",
      " |          >>> encoder = graphlab.feature_engineering.OneHotEncoder(\n",
      " |                  features = ['a', 'b'], max_categories = 2)\n",
      " |      \n",
      " |          # Fit i.e learn the mapping for each column.\n",
      " |          >>> encoder = encoder.fit(sf)\n",
      " |      \n",
      " |          # Return the indices in the encoding.\n",
      " |          >>> encoder['feature_encoding']\n",
      " |      \n",
      " |          Columns:\n",
      " |                    feature    str\n",
      " |                    category    str\n",
      " |                    index    int\n",
      " |      \n",
      " |          Rows: 4\n",
      " |      \n",
      " |          Data:\n",
      " |          +---------+----------+-------+\n",
      " |          | feature | category | index |\n",
      " |          +---------+----------+-------+\n",
      " |          |    a    |    2     |   0   |\n",
      " |          |    a    |    3     |   1   |\n",
      " |          |    b    |    2     |   2   |\n",
      " |          |    b    |    3     |   3   |\n",
      " |          +---------+----------+-------+\n",
      " |          [4 rows x 3 columns]\n",
      " |  \n",
      " |  fit_transform(self, data)\n",
      " |      First fit a transformer using the SFrame `data` and then return a\n",
      " |      transformed version of `data`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : SFrame\n",
      " |          The data used to fit the transformer. The same data is then also\n",
      " |          transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Transformed SFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      fit, transform\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      - Fit transform modifies self.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: python\n",
      " |      \n",
      " |      \n",
      " |          # Create the data\n",
      " |          >>> from graphlab.toolkits.feature_engineering import OneHotEncoder\n",
      " |          >>> sf = graphlab.SFrame({'a' : [1,2,3,2,3], 'b' : [2,3,4,2,3]})\n",
      " |      \n",
      " |          # Create a one-hot encoder for the features ['a', 'b'].\n",
      " |          >>> encoder = graphlab.feature_engineering.create(sf,\n",
      " |                          OneHotEncoder(features = ['a', 'b']))\n",
      " |      \n",
      " |          # Transform the data.\n",
      " |          >>> transformed_sf = encoder.transform(sf)\n",
      " |          Columns:\n",
      " |              feature    str\n",
      " |              category   str\n",
      " |              index      int\n",
      " |      \n",
      " |          Rows: 4\n",
      " |      \n",
      " |          Data:\n",
      " |          +---------+----------+-------+\n",
      " |          | feature | category | index |\n",
      " |          +---------+----------+-------+\n",
      " |          |    a    |    2     |   0   |\n",
      " |          |    a    |    3     |   1   |\n",
      " |          |    b    |    2     |   2   |\n",
      " |          |    b    |    3     |   3   |\n",
      " |          +---------+----------+-------+\n",
      " |          [4 rows x 3 columns]\n",
      " |      \n",
      " |          # Use the top-2 most frequent categories per column for features ['a', 'b'].\n",
      " |          >>> encoder = graphlab.feature_engineering.OneHotEncoder(\n",
      " |                  features = ['a', 'b'], max_categories = 2)\n",
      " |      \n",
      " |          # Fit and transform on the same data.\n",
      " |          >>> transformed_sf = encoder.fit_transform(sf)\n",
      " |          Columns:\n",
      " |              encoded_features    dict\n",
      " |      \n",
      " |          Rows: 5\n",
      " |      \n",
      " |          Data:\n",
      " |          +------------------+\n",
      " |          | encoded_features |\n",
      " |          +------------------+\n",
      " |          |      {2: 1}      |\n",
      " |          |   {0: 1, 3: 1}   |\n",
      " |          |      {1: 1}      |\n",
      " |          |   {0: 1, 2: 1}   |\n",
      " |          |   {1: 1, 3: 1}   |\n",
      " |          +------------------+\n",
      " |          [5 rows x 1 columns]\n",
      " |  \n",
      " |  transform(self, data)\n",
      " |      Transform the SFrame `data` using a fitted model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : SFrame\n",
      " |          The data  to be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A transformed SFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      fit, fit_transform\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: python\n",
      " |      \n",
      " |      \n",
      " |          # String/Integer columns\n",
      " |          # ----------------------------------------------------------------------\n",
      " |          >>> from graphlab.toolkits.feature_engineering import OneHotEncoder\n",
      " |          >>> sf = graphlab.SFrame({'a' : [1,2,3,2,3], 'b' : [2,3,4,2,3]})\n",
      " |      \n",
      " |          # Create a OneHotEncoder for features ['a', 'b']\n",
      " |          >>> encoder = graphlab.feature_engineering.create(sf,\n",
      " |                  OneHotEncoder(features = ['a', 'b']))\n",
      " |      \n",
      " |          # Fit and transform on the same data.\n",
      " |          >>> transformed_sf = encoder.fit_transform(sf)\n",
      " |          Columns:\n",
      " |              encoded_features    dict\n",
      " |      \n",
      " |          Rows: 5\n",
      " |      \n",
      " |          Data:\n",
      " |          +--------------------+\n",
      " |          |  encoded_features  |\n",
      " |          +--------------------+\n",
      " |          | {0: 1, 1: 1, 2: 1} |\n",
      " |          | {2: 1, 3: 1, 4: 1} |\n",
      " |          | {5: 1, 6: 1, 7: 1} |\n",
      " |          | {2: 1, 3: 1, 4: 1} |\n",
      " |          | {0: 1, 3: 1, 6: 1} |\n",
      " |          +--------------------+\n",
      " |          [5 rows x 1 columns]\n",
      " |      \n",
      " |          # Lists can be used to encode sets of categories for each example.\n",
      " |          # ----------------------------------------------------------------------\n",
      " |          >>> from graphlab.toolkits.feature_engineering import OneHotEncoder\n",
      " |          >>> sf = graphlab.SFrame({'categories': [['cat', 'mammal'],\n",
      " |                                                   ['dog', 'mammal'],\n",
      " |                                                   ['human', 'mammal'],\n",
      " |                                                   ['seahawk', 'bird'],\n",
      " |                                                   ['wasp', 'insect']]})\n",
      " |      \n",
      " |          # Construct and fit.\n",
      " |          >>> encoder = graphlab.feature_engineering.create(sf,\n",
      " |                      OneHotEncoder(features = ['categories']))\n",
      " |      \n",
      " |          # Transform the data\n",
      " |          >>> transformed_sf = encoder.transform(sf)\n",
      " |          Columns:\n",
      " |              encoded_features    dict\n",
      " |      \n",
      " |          Rows: 5\n",
      " |      \n",
      " |          Data:\n",
      " |          +------------------+\n",
      " |          | encoded_features |\n",
      " |          +------------------+\n",
      " |          |   {0: 1, 1: 1}   |\n",
      " |          |   {0: 1, 2: 1}   |\n",
      " |          |   {0: 1, 3: 1}   |\n",
      " |          |   {4: 1, 5: 1}   |\n",
      " |          |   {6: 1, 7: 1}   |\n",
      " |          +------------------+\n",
      " |          [5 rows x 1 columns]\n",
      " |      \n",
      " |          # Dictionaries can be used for name spaces & sub-categories.\n",
      " |          # ----------------------------------------------------------------------\n",
      " |          >>> from graphlab.toolkits.feature_engineering import OneHotEncoder\n",
      " |          >>> sf = graphlab.SFrame({'attributes':\n",
      " |                          [{'height':'tall', 'age': 'senior', 'weight': 'thin'},\n",
      " |                           {'height':'short', 'age': 'child', 'weight': 'thin'},\n",
      " |                           {'height':'giant', 'age': 'adult', 'weight': 'fat'},\n",
      " |                           {'height':'short', 'age': 'child', 'weight': 'thin'},\n",
      " |                           {'height':'tall', 'age': 'child', 'weight': 'fat'}]})\n",
      " |      \n",
      " |          # Construct and fit.\n",
      " |          >>> encoder = graphlab.feature_engineering.create(sf,\n",
      " |                                  OneHotEncoder(features = ['attributes']))\n",
      " |      \n",
      " |          # Transform the data\n",
      " |          >>> transformed_sf = encoder.transform(sf)\n",
      " |          Columns:\n",
      " |              encoded_features    dict\n",
      " |      \n",
      " |          Rows: 5\n",
      " |      \n",
      " |          Data:\n",
      " |          +--------------------+\n",
      " |          |  encoded_features  |\n",
      " |          +--------------------+\n",
      " |          | {0: 1, 1: 1, 2: 1} |\n",
      " |          | {2: 1, 3: 1, 4: 1} |\n",
      " |          | {5: 1, 6: 1, 7: 1} |\n",
      " |          | {2: 1, 3: 1, 4: 1} |\n",
      " |          | {0: 1, 3: 1, 6: 1} |\n",
      " |          +--------------------+\n",
      " |          [5 rows x 1 columns]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_default_options = get_default_options_for_model(output_type='sframe')\n",
      " |      Get the default options for the toolkit\n",
      " |      :class:`~graphlab.toolkits.feature_engineering._one_hot_encoder.OneHotEncoder`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output_type : str, optional\n",
      " |      \n",
      " |          The output can be of the following types.\n",
      " |      \n",
      " |          - `sframe`: A table description each option used in the model.\n",
      " |          - `json`: A list of option dictionaries suitable for JSON serialization.\n",
      " |      \n",
      " |          | Each dictionary/row in the dictionary/SFrame object describes the\n",
      " |            following parameters of the given model.\n",
      " |      \n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          |      Name        |                  Description                          |\n",
      " |          +==================+=======================================================+\n",
      " |          | name             | Name of the option used in the model.                 |\n",
      " |          +------------------+---------+---------------------------------------------+\n",
      " |          | description      | A detailed description of the option used.            |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          | type             | Option type (REAL, BOOL, INTEGER or CATEGORICAL)      |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          | default_value    | The default value for the option.                     |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          | possible_values  | List of acceptable values (CATEGORICAL only)          |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          | lower_bound      | Smallest acceptable value for this option (REAL only) |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |          | upper_bound      | Largest acceptable value for this option (REAL only)  |\n",
      " |          +------------------+-------------------------------------------------------+\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : dict/SFrame\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      graphlab.toolkits.feature_engineering._one_hot_encoder.OneHotEncoder.get_current_options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: python\n",
      " |      \n",
      " |        >>> import graphlab\n",
      " |      \n",
      " |        # SFrame formatted output.\n",
      " |        >>> out_sframe = graphlab.toolkits.feature_engineering._one_hot_encoder.get_default_options()\n",
      " |      \n",
      " |        # dict formatted output suitable for JSON serialization.\n",
      " |        >>> out_sframe = graphlab.toolkits.feature_engineering._one_hot_encoder.get_default_options('json')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from graphlab.toolkits.feature_engineering._feature_engineering.Transformer:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  get(self, field)\n",
      " |      Return the value for the queried field.\n",
      " |      \n",
      " |      Each of these fields can be queried in one of two ways:\n",
      " |      \n",
      " |      >>> out = m['field']\n",
      " |      >>> out = m.get('field')  # equivalent to previous line\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : string\n",
      " |          Name of the field to be retrieved.\n",
      " |      \n",
      " |      See Also\n",
      " |      ---------\n",
      " |      list_fields\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : value\n",
      " |          The current value of the requested field.\n",
      " |  \n",
      " |  get_current_options(self)\n",
      " |      Return a dictionary with the options used to define and train the\n",
      " |      transformer.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : dict\n",
      " |          Dictionary with options used to define and train the transformer.\n",
      " |      \n",
      " |      See Also\n",
      " |      ---------\n",
      " |      get, list_fields\n",
      " |      \n",
      " |      Examples\n",
      " |      ----------\n",
      " |      >>> options = model.get_current_options()\n",
      " |  \n",
      " |  list_fields(self)\n",
      " |      List of fields stored in the model. Each of these fields can be queried\n",
      " |      using the ``get(field)`` function or ``m[field]``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : list[str]\n",
      " |          A list of fields that can be queried using the ``get`` method.\n",
      " |      \n",
      " |      See Also\n",
      " |      ---------\n",
      " |      get\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> fields = m.list_fields()\n",
      " |  \n",
      " |  save(self, location)\n",
      " |      Save the transformer into a GraphLab archive. The object is saved as a\n",
      " |      directory which can then be loaded using the\n",
      " |      :py:func:`~graphlab.load_model` method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      location : string\n",
      " |          Target destination for the model. Can be a local path or remote\n",
      " |          URL.\n",
      " |      \n",
      " |      See Also\n",
      " |      ----------\n",
      " |      graphlab.load_model\n",
      " |      \n",
      " |      Examples\n",
      " |      ----------\n",
      " |      .. sourcecode:: python\n",
      " |      \n",
      " |          >>> model.save('my_model_file')\n",
      " |          >>> loaded_model = gl.load_model('my_model_file')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from graphlab.toolkits._model.CustomModel:\n",
      " |  \n",
      " |  show(self, view=None, model_type='base')\n",
      " |      show(view=None)\n",
      " |      Visualize with GraphLab Canvas :mod:`~graphlab.canvas`. This function\n",
      " |      starts Canvas if it is not already running. If the model has already\n",
      " |      been plotted, this function will update the plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      view : str, optional\n",
      " |      \n",
      " |          - 'Summary': The summary description of a Model.\n",
      " |      \n",
      " |          - 'Evaluation': A visual representation of the evaluation results\n",
      " |            for a Model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      view : graphlab.canvas.view.View\n",
      " |          An object representing the GraphLab Canvas view.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      canvas\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Suppose 'm' is a Model, we can view it in GraphLab Canvas using:\n",
      " |      \n",
      " |      >>> m.show()\n",
      " |  \n",
      " |  summary(self, output=None)\n",
      " |      Print a summary of the model. The summary includes a description of\n",
      " |      training data, options, hyper-parameters, and statistics measured\n",
      " |      during model creation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      output : str, None\n",
      " |          The type of summary to return.\n",
      " |      \n",
      " |          - None or 'stdout' : print directly to stdout.\n",
      " |      \n",
      " |          - 'str' : string of summary\n",
      " |      \n",
      " |          - 'dict' : a dict with 'sections' and 'section_titles' ordered\n",
      " |            lists. The entries in the 'sections' list are tuples of the form\n",
      " |            ('label', 'value').\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m.summary()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from graphlab.toolkits._model.CustomModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(graphlab.feature_engineering.OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dict_keys in module graphlab.data_structures.sarray:\n",
      "\n",
      "dict_keys(self) method of graphlab.data_structures.sarray.SArray instance\n",
      "    Create an SArray that contains all the keys from each dictionary\n",
      "    element as a list. Fails on SArrays whose data type is not ``dict``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : SArray\n",
      "        A SArray of list type, where each element is a list of keys\n",
      "        from the input SArray element.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    dict_values\n",
      "    \n",
      "    Examples\n",
      "    ---------\n",
      "    >>> sa = graphlab.SArray([{\"this\":1, \"is\":5, \"dog\":7},\n",
      "                              {\"this\": 2, \"are\": 1, \"cat\": 5}])\n",
      "    >>> sa.dict_keys()\n",
      "    dtype: list\n",
      "    Rows: 2\n",
      "    [['this', 'is', 'dog'], ['this', 'are', 'cat']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = wiki['tf_idf']\n",
    "help(a.dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sframe_to_scipy(column):\n",
    "    \"\"\" \n",
    "    Convert a dict-typed SArray into a SciPy sparse matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        mat : a SciPy sparse matrix where mat[i, j] is the value of word j for document i.\n",
    "        mapping : a dictionary where mapping[j] is the word whose values are in column j.\n",
    "    \"\"\"\n",
    "    # Create triples of (row_id, feature_id, count).\n",
    "    x = graphlab.SFrame({'X1':column})\n",
    "    \n",
    "    # 1. Add a row number.\n",
    "    x = x.add_row_number()\n",
    "    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n",
    "    x = x.stack('X1', ['feature', 'value'])\n",
    "\n",
    "    # Map words into integers using a OneHotEncoder feature transformation.\n",
    "    f = graphlab.feature_engineering.OneHotEncoder(features=['feature'])\n",
    "\n",
    "    # We first fit the transformer using the above data.\n",
    "    f.fit(x)\n",
    "\n",
    "    # The transform method will add a new column that is the transformed version\n",
    "    # of the 'word' column.\n",
    "    x = f.transform(x)\n",
    "\n",
    "    # Get the feature mapping.\n",
    "    mapping = f['feature_encoding']\n",
    "\n",
    "    # Get the actual word id.\n",
    "    x['feature_id'] = x['encoded_features'].dict_keys().apply(lambda x: x[0])\n",
    "\n",
    "    # Create numpy arrays that contain the data for the sparse matrix.\n",
    "    i = np.array(x['id'])\n",
    "    j = np.array(x['feature_id'])\n",
    "    v = np.array(x['value'])\n",
    "    width = x['id'].max() + 1\n",
    "    height = x['feature_id'].max() + 1\n",
    "\n",
    "    # Create a sparse matrix.\n",
    "    mat = csr_matrix((v, (i, j)), shape=(width, height))\n",
    "\n",
    "    return mat, mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion should take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+------------------+\n",
      "| id |     value      | encoded_features |\n",
      "+----+----------------+------------------+\n",
      "| 0  | 3.89143101194  |   {546634: 1}    |\n",
      "| 0  | 1.56136627032  |   {547901: 1}    |\n",
      "| 0  | 5.51003183729  |   {542919: 1}    |\n",
      "| 0  |  2.4487155642  |   {547689: 1}    |\n",
      "| 0  |  3.7712554105  |   {546778: 1}    |\n",
      "| 0  | 1.63708896913  |   {547889: 1}    |\n",
      "| 0  | 10.9864953892  |    {92219: 1}    |\n",
      "| 0  | 0.462727091616 |   {547958: 1}    |\n",
      "| 0  |  7.8510011733  |   {523996: 1}    |\n",
      "| 0  |  3.992562414   |   {546518: 1}    |\n",
      "+----+----------------+------------------+\n",
      "[10379283 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
      "29.7816319466\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "corpus, mapping = sframe_to_scipy(wiki['tf_idf'])\n",
    "end=time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: The following code block should return 'Check passed correctly', indicating that your matrix contains TF-IDF values for 59071 documents and 547979 unique words.  Otherwise, it will return Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check passed correctly!\n"
     ]
    }
   ],
   "source": [
    "assert corpus.shape == (59071, 547979)\n",
    "print 'Check passed correctly!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSH performs an efficient neighbor search by randomly partitioning all reference data points into different bins. Today we will build a popular variant of LSH known as random binary projection, which approximates cosine distance. There are other variants we could use for other choices of distance metrics.\n",
    "\n",
    "The first step is to generate a collection of random vectors from the standard Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_vectors(num_vector, dim):\n",
    "    return np.random.randn(dim, num_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize these Gaussian random vectors, let's look at an example in low-dimensions.  Below, we generate 3 random vectors each of dimension 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate 3 random vectors of dimension 5, arranged into a single 5 x 3 matrix.\n",
    "np.random.seed(0) # set seed=0 for consistent results\n",
    "generate_random_vectors(num_vector=3, dim=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate random vectors of the same dimensionality as our vocubulary size (547979).  Each vector can be used to compute one bit in the bin encoding.  We generate 16 vectors, leading to a 16-bit encoding of the bin index for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate 16 random vectors of dimension 547979\n",
    "np.random.seed(0)\n",
    "random_vectors = generate_random_vectors(num_vector=16, dim=547979)\n",
    "random_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we partition data points into bins. Instead of using explicit loops, we'd like to utilize matrix operations for greater efficiency. Let's walk through the construction step by step.\n",
    "\n",
    "We'd like to decide which bin document 0 should go. Since 16 random vectors were generated in the previous cell, we have 16 bits to represent the bin index. The first bit is given by the sign of the dot product between the first random vector and the document's TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = corpus[0, :] # vector of tf-idf values for document 0\n",
    "doc.dot(random_vectors[:, 0]) >= 0 # True if positive sign; False if negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the second bit is computed as the sign of the dot product between the second random vector and the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.dot(random_vectors[:, 1]) >= 0 # True if positive sign; False if negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute all of the bin index bits at once as follows. Note the absence of the explicit `for` loop over the 16 vectors. Matrix operations let us batch dot-product computation in a highly efficent manner, unlike the `for` loop construction. Given the relative inefficiency of loops in Python, the advantage of matrix operations is even greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc.dot(random_vectors) >= 0 # should return an array of 16 True/False bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(doc.dot(random_vectors) >= 0, dtype=int) # display index bits in 0/1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents that obtain exactly this vector will be assigned to the same bin. We'd like to repeat the identical operation on all documents in the Wikipedia dataset and compute the corresponding bin indices. Again, we use matrix operations  so that no explicit loop is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus[0:2].dot(random_vectors) >= 0 # compute bit indices of first two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.dot(random_vectors) >= 0 # compute bit indices of ALL documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! To make it convenient to refer to individual bins, we convert each binary bin index into a single integer: \n",
    "```\n",
    "Bin index                      integer\n",
    "[0,0,0,0,0,0,0,0,0,0,0,0]   => 0\n",
    "[0,0,0,0,0,0,0,0,0,0,0,1]   => 1\n",
    "[0,0,0,0,0,0,0,0,0,0,1,0]   => 2\n",
    "[0,0,0,0,0,0,0,0,0,0,1,1]   => 3\n",
    "...\n",
    "[1,1,1,1,1,1,1,1,1,1,0,0]   => 65532\n",
    "[1,1,1,1,1,1,1,1,1,1,0,1]   => 65533\n",
    "[1,1,1,1,1,1,1,1,1,1,1,0]   => 65534\n",
    "[1,1,1,1,1,1,1,1,1,1,1,1]   => 65535 (= 2^16-1)\n",
    "```\n",
    "By the [rules of binary number representation](https://en.wikipedia.org/wiki/Binary_number#Decimal), we just need to compute the dot product between the document vector and the vector consisting of powers of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = corpus[0, :]  # first document\n",
    "index_bits = (doc.dot(random_vectors) >= 0)\n",
    "powers_of_two = (1 << np.arange(15, -1, -1))\n",
    "print index_bits\n",
    "print powers_of_two\n",
    "print index_bits.dot(powers_of_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's the dot product again, we batch it with a matrix operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_bits = corpus.dot(random_vectors) >= 0\n",
    "index_bits.dot(powers_of_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array gives us the integer index of the bins for all documents.\n",
    "\n",
    "Now we are ready to complete the following function. Given the integer bin indices for the documents, you should compile a list of document IDs that belong to each bin. Since a list is to be maintained for each unique bin index, a dictionary of lists is used.\n",
    "\n",
    "1. Compute the integer bin indices. This step is already completed.\n",
    "2. For each document in the dataset, do the following:\n",
    "   * Get the integer bin index for the document.\n",
    "   * Fetch the list of document ids associated with the bin; if no list yet exists for this bin, assign the bin an empty list.\n",
    "   * Add the document id to the end of the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_lsh(data, num_vector=16, seed=None):\n",
    "    \n",
    "    dim = data.shape[1]\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    random_vectors = generate_random_vectors(num_vector, dim)\n",
    "  \n",
    "    powers_of_two = 1 << np.arange(num_vector-1, -1, -1)\n",
    "  \n",
    "    table = {}\n",
    "    \n",
    "    # Partition data points into bins\n",
    "    bin_index_bits = (data.dot(random_vectors) >= 0)\n",
    "  \n",
    "    # Encode bin index bits into integers\n",
    "    bin_indices = bin_index_bits.dot(powers_of_two)\n",
    "    \n",
    "    # Update `table` so that `table[i]` is the list of document ids with bin index equal to i.\n",
    "    for data_index, bin_index in enumerate(bin_indices):\n",
    "        if bin_index not in table:\n",
    "            # If no list yet exists for this bin, assign the bin an empty list.\n",
    "            table[bin_index] = ... # YOUR CODE HERE\n",
    "        # Fetch the list of document ids associated with the bin and add the document id to the end.\n",
    "        ... # YOUR CODE HERE\n",
    "\n",
    "    model = {'data': data,\n",
    "             'bin_index_bits': bin_index_bits,\n",
    "             'bin_indices': bin_indices,\n",
    "             'table': table,\n",
    "             'random_vectors': random_vectors,\n",
    "             'num_vector': num_vector}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_lsh(corpus, num_vector=16, seed=143)\n",
    "table = model['table']\n",
    "if   0 in table and table[0]   == [39583] and \\\n",
    "   143 in table and table[143] == [19693, 28277, 29776, 30399]:\n",
    "    print 'Passed!'\n",
    "else:\n",
    "    print 'Check your code.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** We will be using the model trained here in the following sections, unless otherwise indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some documents and see which bins they fall into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki[wiki['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. What is the document `id` of Barack Obama's article?\n",
    "\n",
    "**Quiz Question**. Which bin contains Barack Obama's article? Enter its integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous assignment that Joe Biden was a close neighbor of Barack Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki[wiki['name'] == 'Joe Biden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Examine the bit representations of the bins containing Barack Obama and Joe Biden. In how many places do they agree?\n",
    "\n",
    "1. 16 out of 16 places (Barack Obama and Joe Biden fall into the same bin)\n",
    "2. 14 out of 16 places\n",
    "3. 12 out of 16 places\n",
    "4. 10 out of 16 places\n",
    "5. 8 out of 16 places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result with a former British diplomat, whose bin representation agrees with Obama's in only 8 out of 16 places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki[wiki['name']=='Wynn Normington Hugh-Jones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.array(model['bin_index_bits'][22745], dtype=int) # list of 0/1's\n",
    "print model['bin_indices'][22745] # integer format\n",
    "model['bin_index_bits'][35817] == model['bin_index_bits'][22745]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the documents in the same bin as Barack Obama? Are they necessarily more similar to Obama than Biden?  Let's look at which documents are in the same bin as the Barack Obama article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model['table'][model['bin_indices'][35817]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four other documents that belong to the same bin. Which documents are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_ids = list(model['table'][model['bin_indices'][35817]])\n",
    "doc_ids.remove(35817) # display documents other than Obama\n",
    "\n",
    "docs = wiki.filter_by(values=doc_ids, column_name='id') # filter by id column\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Joe Biden is much closer to Barack Obama than any of the four documents, even though Biden's bin representation differs from Obama's by 2 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    xy = x.dot(y.T)\n",
    "    dist = xy/(norm(x)*norm(y))\n",
    "    return 1-dist[0,0]\n",
    "\n",
    "obama_tf_idf = corpus[35817,:]\n",
    "biden_tf_idf = corpus[24478,:]\n",
    "\n",
    "print '================= Cosine distance from Barack Obama'\n",
    "print 'Barack Obama - {0:24s}: {1:f}'.format('Joe Biden',\n",
    "                                             cosine_distance(obama_tf_idf, biden_tf_idf))\n",
    "for doc_id in doc_ids:\n",
    "    doc_tf_idf = corpus[doc_id,:]\n",
    "    print 'Barack Obama - {0:24s}: {1:f}'.format(wiki[doc_id]['name'],\n",
    "                                                 cosine_distance(obama_tf_idf, doc_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moral of the story**. Similar data points will in general _tend to_ fall into _nearby_ bins, but that's all we can say about LSH. In a high-dimensional space such as text features, we often get unlucky with our selection of only a few random vectors such that dissimilar data points go into the same bin while similar data points fall into different bins. **Given a query document, we must consider all documents in the nearby bins and sort them according to their actual distances from the query.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first implement the logic for searching nearby neighbors, which goes like this:\n",
    "```\n",
    "1. Let L be the bit representation of the bin that contains the query documents.\n",
    "2. Consider all documents in bin L.\n",
    "3. Consider documents in the bins whose bit representation differs from L by 1 bit.\n",
    "4. Consider documents in the bins whose bit representation differs from L by 2 bits.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain candidate bins that differ from the query bin by some number of bits, we use `itertools.combinations`, which produces all possible subsets of a given list. See [this documentation](https://docs.python.org/3/library/itertools.html#itertools.combinations) for details.\n",
    "```\n",
    "1. Decide on the search radius r. This will determine the number of different bits between the two vectors.\n",
    "2. For each subset (n_1, n_2, ..., n_r) of the list [0, 1, 2, ..., num_vector-1], do the following:\n",
    "   * Flip the bits (n_1, n_2, ..., n_r) of the query bin to produce a new bit vector.\n",
    "   * Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "   * Add those documents to the candidate set.\n",
    "```\n",
    "\n",
    "Each line of output from the following cell is a 3-tuple indicating where the candidate bin would differ from the query bin. For instance,\n",
    "```\n",
    "(0, 1, 3)\n",
    "```\n",
    "indicates that the candiate bin differs from the query bin in first, second, and fourth bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_vector = 16\n",
    "search_radius = 3\n",
    "\n",
    "for diff in combinations(range(num_vector), search_radius):\n",
    "    print diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this output in mind, implement the logic for nearby bin search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_nearby_bins(query_bin_bits, table, search_radius=2, initial_candidates=set()):\n",
    "    \"\"\"\n",
    "    For a given query vector and trained LSH model, return all candidate neighbors for\n",
    "    the query among all bins within the given search radius.\n",
    "    \n",
    "    Example usage\n",
    "    -------------\n",
    "    >>> model = train_lsh(corpus, num_vector=16, seed=143)\n",
    "    >>> q = model['bin_index_bits'][0]  # vector for the first document\n",
    "  \n",
    "    >>> candidates = search_nearby_bins(q, model['table'])\n",
    "    \"\"\"\n",
    "    num_vector = len(query_bin_bits)\n",
    "    powers_of_two = 1 << np.arange(num_vector-1, -1, -1)\n",
    "    \n",
    "    # Allow the user to provide an initial set of candidates.\n",
    "    candidate_set = copy(initial_candidates)\n",
    "    \n",
    "    for different_bits in combinations(range(num_vector), search_radius):       \n",
    "        # Flip the bits (n_1,n_2,...,n_r) of the query bin to produce a new bit vector.\n",
    "        ## Hint: you can iterate over a tuple like a list\n",
    "        alternate_bits = copy(query_bin_bits)\n",
    "        for i in different_bits:\n",
    "            alternate_bits[i] = ... # YOUR CODE HERE \n",
    "        \n",
    "        # Convert the new bit vector to an integer index\n",
    "        nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "        \n",
    "        # Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "        # Then add those documents to candidate_set\n",
    "        # Make sure that the bin exists in the table!\n",
    "        # Hint: update() method for sets lets you add an entire list to the set\n",
    "        if nearby_bin in table:\n",
    "            ... # YOUR CODE HERE: Update candidate_set with the documents in this bin.\n",
    "            \n",
    "    return candidate_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Running the function with `search_radius=0` should yield the list of documents belonging to the same bin as the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama_bin_index = model['bin_index_bits'][35817] # bin index of Barack Obama\n",
    "candidate_set = search_nearby_bins(obama_bin_index, model['table'], search_radius=0)\n",
    "if candidate_set == set([35817, 21426, 53937, 39426, 50261]):\n",
    "    print 'Passed test'\n",
    "else:\n",
    "    print 'Check your code'\n",
    "print 'List of documents in the same bin as Obama: 35817, 21426, 53937, 39426, 50261'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Running the function with `search_radius=1` adds more documents to the fore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_set = search_nearby_bins(obama_bin_index, model['table'], search_radius=1, initial_candidates=candidate_set)\n",
    "if candidate_set == set([39426, 38155, 38412, 28444, 9757, 41631, 39207, 59050, 47773, 53937, 21426, 34547,\n",
    "                         23229, 55615, 39877, 27404, 33996, 21715, 50261, 21975, 33243, 58723, 35817, 45676,\n",
    "                         19699, 2804, 20347]):\n",
    "    print 'Passed test'\n",
    "else:\n",
    "    print 'Check your code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. Don't be surprised if few of the candidates look similar to Obama. This is why we add as many candidates as our computational budget allows and sort them by their distance to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that can return all the candidates from neighboring bins. Next we write a function to collect all candidates and compute their true distance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query(vec, model, k, max_search_radius):\n",
    "  \n",
    "    data = model['data']\n",
    "    table = model['table']\n",
    "    random_vectors = model['random_vectors']\n",
    "    num_vector = random_vectors.shape[1]\n",
    "    \n",
    "    \n",
    "    # Compute bin index for the query vector, in bit representation.\n",
    "    bin_index_bits = (vec.dot(random_vectors) >= 0).flatten()\n",
    "    \n",
    "    # Search nearby bins and collect candidates\n",
    "    candidate_set = set()\n",
    "    for search_radius in xrange(max_search_radius+1):\n",
    "        candidate_set = search_nearby_bins(bin_index_bits, table, search_radius, initial_candidates=candidate_set)\n",
    "    \n",
    "    # Sort candidates by their true distances from the query\n",
    "    nearest_neighbors = graphlab.SFrame({'id':candidate_set})\n",
    "    candidates = data[np.array(list(candidate_set)),:]\n",
    "    nearest_neighbors['distance'] = pairwise_distances(candidates, vec, metric='cosine').flatten()\n",
    "    \n",
    "    return nearest_neighbors.topk('distance', k, reverse=True), len(candidate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query(corpus[35817,:], model, k=10, max_search_radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the documents, it's helpful to join this table with the Wikipedia table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query(corpus[35817,:], model, k=10, max_search_radius=3)[0].join(wiki[['id', 'name']], on='id').sort('distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown that we have a working LSH implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with your LSH implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we have implemented a few experiments so that you can gain intuition for how your LSH implementation behaves in different situations. This will help you understand the effect of searching nearby bins and the performance of LSH versus computing nearest neighbors using a brute force search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of nearby bin search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does nearby bin search affect the outcome of LSH? There are three variables that are affected by the search radius:\n",
    "* Number of candidate documents considered\n",
    "* Query time\n",
    "* Distance of approximate neighbors from the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run LSH multiple times, each with different radii for nearby bin search. We will measure the three variables as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki[wiki['name']=='Barack Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidates_history = []\n",
    "query_time_history = []\n",
    "max_distance_from_query_history = []\n",
    "min_distance_from_query_history = []\n",
    "average_distance_from_query_history = []\n",
    "\n",
    "for max_search_radius in xrange(17):\n",
    "    start=time.time()\n",
    "    result, num_candidates = query(corpus[35817,:], model, k=10,\n",
    "                                   max_search_radius=max_search_radius)\n",
    "    end=time.time()\n",
    "    query_time = end-start\n",
    "    \n",
    "    print 'Radius:', max_search_radius\n",
    "    print result.join(wiki[['id', 'name']], on='id').sort('distance')\n",
    "    \n",
    "    average_distance_from_query = result['distance'][1:].mean()\n",
    "    max_distance_from_query = result['distance'][1:].max()\n",
    "    min_distance_from_query = result['distance'][1:].min()\n",
    "    \n",
    "    num_candidates_history.append(num_candidates)\n",
    "    query_time_history.append(query_time)\n",
    "    average_distance_from_query_history.append(average_distance_from_query)\n",
    "    max_distance_from_query_history.append(max_distance_from_query)\n",
    "    min_distance_from_query_history.append(min_distance_from_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the top 10 query results become more relevant as the search radius grows. Let's plot the three variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(num_candidates_history, linewidth=4)\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('# of documents searched')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(query_time_history, linewidth=4)\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(average_distance_from_query_history, linewidth=4, label='Average of 10 neighbors')\n",
    "plt.plot(max_distance_from_query_history, linewidth=4, label='Farthest of 10 neighbors')\n",
    "plt.plot(min_distance_from_query_history, linewidth=4, label='Closest of 10 neighbors')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Cosine distance of neighbors')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* As we increase the search radius, we find more neighbors that are a smaller distance away.\n",
    "* With increased search radius comes a greater number documents that have to be searched. Query time is higher as a consequence.\n",
    "* With sufficiently high search radius, the results of LSH begin to resemble the results of brute-force search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. What was the smallest search radius that yielded the correct nearest neighbor, namely Joe Biden?\n",
    "\n",
    "\n",
    "**Quiz Question**. Suppose our goal was to produce 10 approximate nearest neighbors whose average distance from the query document is within 0.01 of the average for the true 10 nearest neighbors. For Barack Obama, the true 10 nearest neighbors are on average about 0.77. What was the smallest search radius for Barack Obama that produced an average distance of 0.78 or better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality metrics for neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis is limited by the fact that it was run with a single query, namely Barack Obama. We should repeat the analysis for the entirety of data. Iterating over all documents would take a long time, so let us randomly choose 10 documents for our analysis.\n",
    "\n",
    "For each document, we first compute the true 25 nearest neighbors, and then run LSH multiple times. We look at two metrics:\n",
    "\n",
    "* Precision@10: How many of the 10 neighbors given by LSH are among the true 25 nearest neighbors?\n",
    "* Average cosine distance of the neighbors from the query\n",
    "\n",
    "Then we run LSH multiple times with different search radii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brute_force_query(vec, data, k):\n",
    "    num_data_points = data.shape[0]\n",
    "    \n",
    "    # Compute distances for ALL data points in training set\n",
    "    nearest_neighbors = graphlab.SFrame({'id':range(num_data_points)})\n",
    "    nearest_neighbors['distance'] = pairwise_distances(data, vec, metric='cosine').flatten()\n",
    "    \n",
    "    return nearest_neighbors.topk('distance', k, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run LSH with multiple search radii and compute the quality metrics for each run. Allow a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_radius = 17\n",
    "precision = {i:[] for i in xrange(max_radius)}\n",
    "average_distance  = {i:[] for i in xrange(max_radius)}\n",
    "query_time  = {i:[] for i in xrange(max_radius)}\n",
    "\n",
    "np.random.seed(0)\n",
    "num_queries = 10\n",
    "for i, ix in enumerate(np.random.choice(corpus.shape[0], num_queries, replace=False)):\n",
    "    print('%s / %s' % (i, num_queries))\n",
    "    ground_truth = set(brute_force_query(corpus[ix,:], corpus, k=25)['id'])\n",
    "    # Get the set of 25 true nearest neighbors\n",
    "    \n",
    "    for r in xrange(1,max_radius):\n",
    "        start = time.time()\n",
    "        result, num_candidates = query(corpus[ix,:], model, k=10, max_search_radius=r)\n",
    "        end = time.time()\n",
    "\n",
    "        query_time[r].append(end-start)\n",
    "        # precision = (# of neighbors both in result and ground_truth)/10.0\n",
    "        precision[r].append(len(set(result['id']) & ground_truth)/10.0)\n",
    "        average_distance[r].append(result['distance'][1:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(average_distance[i]) for i in xrange(1,17)], linewidth=4, label='Average over 10 neighbors')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Cosine distance')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(precision[i]) for i in xrange(1,17)], linewidth=4, label='Precison@10')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(1,17), [np.mean(query_time[i]) for i in xrange(1,17)], linewidth=4, label='Query time')\n",
    "plt.xlabel('Search radius')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations for Barack Obama generalize to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of number of random vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now turn our focus to the remaining parameter: the number of random vectors. We run LSH with different number of random vectors, ranging from 5 to 20. We fix the search radius to 3.\n",
    "\n",
    "Allow a few minutes for the following cell to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision = {i:[] for i in xrange(5,20)}\n",
    "average_distance  = {i:[] for i in xrange(5,20)}\n",
    "query_time = {i:[] for i in xrange(5,20)}\n",
    "num_candidates_history = {i:[] for i in xrange(5,20)}\n",
    "ground_truth = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "num_queries = 10\n",
    "docs = np.random.choice(corpus.shape[0], num_queries, replace=False)\n",
    "\n",
    "for i, ix in enumerate(docs):\n",
    "    ground_truth[ix] = set(brute_force_query(corpus[ix,:], corpus, k=25)['id'])\n",
    "    # Get the set of 25 true nearest neighbors\n",
    "\n",
    "for num_vector in xrange(5,20):\n",
    "    print('num_vector = %s' % (num_vector))\n",
    "    model = train_lsh(corpus, num_vector, seed=143)\n",
    "    \n",
    "    for i, ix in enumerate(docs):\n",
    "        start = time.time()\n",
    "        result, num_candidates = query(corpus[ix,:], model, k=10, max_search_radius=3)\n",
    "        end = time.time()\n",
    "        \n",
    "        query_time[num_vector].append(end-start)\n",
    "        precision[num_vector].append(len(set(result['id']) & ground_truth[ix])/10.0)\n",
    "        average_distance[num_vector].append(result['distance'][1:].mean())\n",
    "        num_candidates_history[num_vector].append(num_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(average_distance[i]) for i in xrange(5,20)], linewidth=4, label='Average over 10 neighbors')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Cosine distance')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(precision[i]) for i in xrange(5,20)], linewidth=4, label='Precison@10')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(query_time[i]) for i in xrange(5,20)], linewidth=4, label='Query time (seconds)')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('Query time (seconds)')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(range(5,20), [np.mean(num_candidates_history[i]) for i in xrange(5,20)], linewidth=4,\n",
    "         label='# of documents searched')\n",
    "plt.xlabel('# of random vectors')\n",
    "plt.ylabel('# of documents searched')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar trade-off between quality and performance: as the number of random vectors increases, the query time goes down as each bin contains fewer documents on average, but on average the neighbors are likewise placed farther from the query. On the other hand, when using a small enough number of random vectors, LSH becomes very similar brute-force search: Many documents appear in a single bin, so searching the query bin alone covers a lot of the corpus; then, including neighboring bins might result in searching all documents, just as in the brute-force approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
